{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Paths to your PDF files\n",
    "Machine_Learning_path = r'C:\\Users\\Sanskruti\\Desktop\\StepsAI\\Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf'\n",
    "speech_processing_path = r'C:\\Users\\Sanskruti\\Desktop\\StepsAI\\Speech and language processing.pdf'\n",
    "pattern_recognition_path = r'C:\\Users\\Sanskruti\\Desktop\\StepsAI\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf'\n",
    "\n",
    "# Extract text from the PDFs\n",
    "Machine_learning_text = extract_text_from_pdf(Machine_Learning_path)\n",
    "speech_processing_text = extract_text_from_pdf(speech_processing_path)\n",
    "pattern_recognition_text = extract_text_from_pdf(pattern_recognition_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andreas C. Müller & Sarah GuidoIntroduction to  \n",
      "Machine \n",
      "Learning  \n",
      "with P y thon   \n",
      "A GUIDE FOR DATA SCIENTISTS\n",
      "Andreas C. Müller and Sarah GuidoIntroduction to Machine Learning\n",
      "with Python\n",
      "A Guide for Data Scientists\n",
      "Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing978-1-449-36941-5\n",
      "[LSI]Introduction to Machine Learning with Python\n",
      "by Andreas C. Müller and Sarah Guido\n",
      "Copyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.Printed in the United States o\n",
      "Speech and Language Processing\n",
      "An Introduction to Natural Language Processing,\n",
      "Computational Linguistics, and Speech Recognition\n",
      "Third Edition draft\n",
      "Daniel Jurafsky\n",
      "Stanford University\n",
      "James H. Martin\n",
      "University of Colorado at Boulder\n",
      "Copyright ©2023. All rights reserved.\n",
      "Draft of February 3, 2024. Comments and typos welcome!Summary of Contents\n",
      "I Fundamental Algorithms for NLP 1\n",
      "1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n",
      "\n",
      "Information Science and Statistics\n",
      "Series Editors:\n",
      "M. Jordan\n",
      "J. Kleinberg\n",
      "B. Scho ¨lkopfInformation Science and Statistics \n",
      "Akaike and Kitagawa: The Practice of Time Series Analysis. \n",
      "Bishop:   Pattern Recognition and Machine Learning. \n",
      "Cowell, Dawid, Lauritzen, and Spiegelhalter:  Probabilistic Networks and\n",
      "Expert Systems. \n",
      "Doucet, de Freitas, and Gordon:  Sequential Monte Carlo Methods in Practice. \n",
      "Fine:  Feedforward Neural Network Methodology. \n",
      "Hawkins and Olwell: Cumulative Sum Charts and C\n"
     ]
    }
   ],
   "source": [
    "print(Machine_learning_text[:500])  # Print the first 500 characters as a sample\n",
    "print(speech_processing_text[:500])\n",
    "print(pattern_recognition_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hierarchical Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_tree = {\n",
    "    'Machine Learning': {\n",
    "        'Chapter 1': {\n",
    "            'Section 1.1': Machine_learning_text.split('Chapter 2')[0],  # Example to split text by chapters\n",
    "            'Section 1.2': '...',  # Add sections similarly\n",
    "        },\n",
    "        'Chapter 2': {\n",
    "            # Add sections here\n",
    "        },\n",
    "        # Add more chapters here\n",
    "    },\n",
    "    'Speech and Language Processing': {\n",
    "        'Chapter 1': {\n",
    "            'Section 1.1': speech_processing_text.split('Chapter 2')[0],  # Example to split text by chapters\n",
    "            'Section 1.2': '...',  # Add sections similarly\n",
    "        },\n",
    "        'Chapter 2': {\n",
    "            # Add sections here\n",
    "        },\n",
    "        # Add more chapters here\n",
    "    },\n",
    "    'Pattern Recognition': {\n",
    "        'Chapter 1': {\n",
    "            'Section 1.1': pattern_recognition_text.split('Chapter 2')[0],  # Example to split text by chapters\n",
    "            'Section 1.2': '...',  # Add sections similarly\n",
    "        },\n",
    "        'Chapter 2': {\n",
    "            # Add sections here\n",
    "        },\n",
    "        # Add more chapters here\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sanskruti\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'auto', 'car', 'proficiency', 'memorise', 'determine', 'discover', 'scholarship', 'automobile', 'machine', 'eruditeness', 'con', 'learn', 'get_wind', 'study', 'learnedness', 'learning', 'larn', 'get_word', 'instruct', 'technique', 'see', 'encyclopaedism', 'take', 'hear', 'watch', 'teach', 'ascertain', 'encyclopedism', 'memorize', 'motorcar', 'acquire', 'techniqu', 'simple_machine', 'erudition', 'acquisition', 'find_out', 'check', 'get_a_line', 'political_machine', 'machin', 'pick_up']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def expand_query(query):\n",
    "    synonyms = []\n",
    "    for word in query.split():\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "    stemmer = PorterStemmer()\n",
    "    expanded_query = set(synonyms + [stemmer.stem(word) for word in query.split()])\n",
    "    return list(expanded_query)\n",
    "\n",
    "expanded_query = expand_query(\"Machine learning techniques\")\n",
    "print(expanded_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanskruti\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c360f4d09b0d416eaec874fc79d72dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanskruti\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sanskruti\\.cache\\huggingface\\hub\\models--sentence-transformers--msmarco-distilbert-base-v4. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec7edac49954662ae4af1c0847f7da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81c7d2fd0c14a64bab284751501ceb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4904ad73a09940898a3b0f7b5fb2ae0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanskruti\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f000dc435fa4abd8e568789e5f21f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/545 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3b22827325451fa5a287733c577bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d416949f22348c09c71cdc1bb0d81a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/319 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be16609fd8cc4bc082195cd91621fcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e043040e2d47789dc796a03f4e1d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f997a8b35d641afb5fc71a14a21bd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771eefccb97e4cc8bf4cfbf52b225467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Advanced machine learning algorithms. (Score: 0.6301)\n",
      "Document: Introduction to machine learning techniques. (Score: 0.5813)\n",
      "Document: Deep learning models for image recognition. (Score: 0.3852)\n",
      "Document: Natural language processing with transformers. (Score: 0.3711)\n",
      "Document: Applications of AI in healthcare. (Score: 0.0889)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "\n",
    "# Define your corpus\n",
    "corpus = [\n",
    "    \"Introduction to machine learning techniques.\",\n",
    "    \"Deep learning models for image recognition.\",\n",
    "    \"Natural language processing with transformers.\",\n",
    "    \"Advanced machine learning algorithms.\",\n",
    "    \"Applications of AI in healthcare.\"\n",
    "]\n",
    "\n",
    "# Define your query\n",
    "query = \"Machine learning techniques\"\n",
    "\n",
    "# Encode the corpus and the query\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# Perform semantic search\n",
    "hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
    "\n",
    "# Print the results\n",
    "for hit in hits[0]:\n",
    "    print(f\"Document: {corpus[hit['corpus_id']]} (Score: {hit['score']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Deep learning models for image recognition.', 'Advanced machine learning algorithms.', 'Natural language processing with transformers.', 'Applications of AI in healthcare.', 'Introduction to machine learning techniques.']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "\n",
    "# Define your corpus\n",
    "corpus = [\n",
    "    \"Introduction to machine learning techniques.\",\n",
    "    \"Deep learning models for image recognition.\",\n",
    "    \"Natural language processing with transformers.\",\n",
    "    \"Advanced machine learning algorithms.\",\n",
    "    \"Applications of AI in healthcare.\"\n",
    "]\n",
    "\n",
    "# Encode the corpus once\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Define a function to retrieve sections from a document\n",
    "def retrieve_sections(doc):\n",
    "    # Placeholder for your hierarchical tree-based section retrieval logic\n",
    "    # For demonstration, we just return the document split into sections\n",
    "    return doc.split('. ')\n",
    "\n",
    "# Define the RAG system function\n",
    "def rag_system(query):\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Perform semantic search\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
    "    \n",
    "    # Retrieve sections from relevant documents\n",
    "    relevant_sections = []\n",
    "    for hit in hits[0]:\n",
    "        doc = corpus[hit['corpus_id']]\n",
    "        sections = retrieve_sections(doc)\n",
    "        relevant_sections.extend(sections)\n",
    "    \n",
    "    return relevant_sections\n",
    "\n",
    "# Example query\n",
    "relevant_sections = rag_system('deep learning applications')\n",
    "print(relevant_sections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models for image recognition\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Initialize the model for semantic search\n",
    "model = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "\n",
    "# Initialize the QA pipeline\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "\n",
    "# Define your corpus\n",
    "corpus = [\n",
    "    \"Introduction to machine learning techniques.\",\n",
    "    \"Deep learning models for image recognition.\",\n",
    "    \"Natural language processing with transformers.\",\n",
    "    \"Advanced machine learning algorithms.\",\n",
    "    \"Applications of AI in healthcare.\"\n",
    "]\n",
    "\n",
    "# Encode the corpus once\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Define a function to retrieve sections from a document\n",
    "def retrieve_sections(doc):\n",
    "    # Placeholder for your hierarchical tree-based section retrieval logic\n",
    "    # For demonstration, we just return the document split into sections\n",
    "    return doc.split('. ')\n",
    "\n",
    "# Define the RAG system function\n",
    "def rag_system(question):\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode(question, convert_to_tensor=True)\n",
    "    \n",
    "    # Perform semantic search\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
    "    \n",
    "    # Retrieve sections from relevant documents\n",
    "    relevant_sections = []\n",
    "    for hit in hits[0]:\n",
    "        doc = corpus[hit['corpus_id']]\n",
    "        sections = retrieve_sections(doc)\n",
    "        relevant_sections.extend(sections)\n",
    "    \n",
    "    # Combine the sections into a single context\n",
    "    context = ' '.join(relevant_sections)\n",
    "    \n",
    "    # Tokenize the input question and context for the QA model\n",
    "    inputs = qa_tokenizer(question, context, return_tensors='pt')\n",
    "    \n",
    "    # Answer the question using the QA model\n",
    "    qa_outputs = qa_model(**inputs)\n",
    "    \n",
    "    # Get the answer span\n",
    "    answer_start_scores = qa_outputs.start_logits\n",
    "    answer_end_scores = qa_outputs.end_logits\n",
    "    \n",
    "    # Find the tokens with the highest start and end scores\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "    \n",
    "    # Get the answer from the tokens\n",
    "    answer = qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example query\n",
    "answer = rag_system('What is deep learning?')\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
